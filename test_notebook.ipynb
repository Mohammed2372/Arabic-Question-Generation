{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "# from transformers import T5Tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_squad = pd.read_excel('Arabic squad/Arabic-SQuAD.xlsx')\n",
    "df_arcd = pd.read_excel('ARCD/arcd-train.xlsx')\n",
    "df_tydiqa = pd.read_csv('tydiqa/tydiqa-arabic.csv')\n",
    "df_mlqa = pd.read_excel('MLQA/MLQA-dev-context-ar-question-ar.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570bce516b8089140040fa42</td>\n",
       "      <td>ما هو ASCII على أساس؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': 'الأبجدية الإنجليزية', 'answer_start...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570bce516b8089140040fa43</td>\n",
       "      <td>كم شخصيات محددة موجودة في كود ASCII؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '128 حرف ا محدد', 'answer_start': 58}]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570bce516b8089140040fa44</td>\n",
       "      <td>كم عدد أحرف التحكم غير الطباعة؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '33 حرف ا تحكم ا غير الطباعة', 'answ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570bce516b8089140040fa45</td>\n",
       "      <td>كم شخصيات قابلة للطباعة؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '95 حرف ا قابلا للطباعة', 'answer_st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570bce516b8089140040fa46</td>\n",
       "      <td>ما هو الفضاء المعروف أيضا باسم ماذا؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': 'رسم ا غير مرئي 223', 'answer_start'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                              question  \\\n",
       "0  570bce516b8089140040fa42                 ما هو ASCII على أساس؟   \n",
       "1  570bce516b8089140040fa43  كم شخصيات محددة موجودة في كود ASCII؟   \n",
       "2  570bce516b8089140040fa44       كم عدد أحرف التحكم غير الطباعة؟   \n",
       "3  570bce516b8089140040fa45              كم شخصيات قابلة للطباعة؟   \n",
       "4  570bce516b8089140040fa46  ما هو الفضاء المعروف أيضا باسم ماذا؟   \n",
       "\n",
       "                                             context  \\\n",
       "0  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "1  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "2  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "3  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "4  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0  [{'text': 'الأبجدية الإنجليزية', 'answer_start...     0  \n",
       "1   [{'text': '128 حرف ا محدد', 'answer_start': 58}]     0  \n",
       "2  [{'text': '33 حرف ا تحكم ا غير الطباعة', 'answ...     0  \n",
       "3  [{'text': '95 حرف ا قابلا للطباعة', 'answer_st...     0  \n",
       "4  [{'text': 'رسم ا غير مرئي 223', 'answer_start'...     0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48344, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>969331847966</td>\n",
       "      <td>- من هو جمال أحمد حمزة خاشقجي؟</td>\n",
       "      <td>جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...</td>\n",
       "      <td>[{'text': 'صحفي وإعلامي', 'answer_start': 73}]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115150665555</td>\n",
       "      <td>- متى ولد جمال أحمد حمزة خاشقجي وتوفي؟ ال</td>\n",
       "      <td>جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...</td>\n",
       "      <td>[{'text': 'حمزة خاشقجي (13 أكتوبر 1958، المدين...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74212080718</td>\n",
       "      <td>- في أي مدينة ولد جمال أحمد حمزة خاشقجي؟ ال</td>\n",
       "      <td>جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...</td>\n",
       "      <td>[{'text': 'المدينة المنورة', 'answer_start': 39}]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>465699296586</td>\n",
       "      <td>- في أي صحيفة قام بكتابة عمود منذ عام 2017؟ ال</td>\n",
       "      <td>جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...</td>\n",
       "      <td>[{'text': 'واشنطن بوست', 'answer_start': 224}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>564177542570</td>\n",
       "      <td>- كيف وصفها في الصحف ووسائل الإعلام الدولية؟ ال</td>\n",
       "      <td>جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...</td>\n",
       "      <td>[{'text': 'وُصف في الصحف وأجهزة الاعلام العالم...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          question  \\\n",
       "0  969331847966                   - من هو جمال أحمد حمزة خاشقجي؟    \n",
       "1  115150665555         - متى ولد جمال أحمد حمزة خاشقجي وتوفي؟ ال   \n",
       "2   74212080718       - في أي مدينة ولد جمال أحمد حمزة خاشقجي؟ ال   \n",
       "3  465699296586    - في أي صحيفة قام بكتابة عمود منذ عام 2017؟ ال   \n",
       "4  564177542570   - كيف وصفها في الصحف ووسائل الإعلام الدولية؟ ال   \n",
       "\n",
       "                                             context  \\\n",
       "0  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
       "1  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
       "2  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
       "3  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
       "4  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0     [{'text': 'صحفي وإعلامي', 'answer_start': 73}]     0  \n",
       "1  [{'text': 'حمزة خاشقجي (13 أكتوبر 1958، المدين...     0  \n",
       "2  [{'text': 'المدينة المنورة', 'answer_start': 39}]     0  \n",
       "3     [{'text': 'واشنطن بوست', 'answer_start': 224}]     1  \n",
       "4  [{'text': 'وُصف في الصحف وأجهزة الاعلام العالم...     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arcd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arabic-2387335860751143628-1</td>\n",
       "      <td>كم عدد مرات فوز الأوروغواي ببطولة كاس العالم ل...</td>\n",
       "      <td>أقيمت البطولة 21 مرة، شارك في النهائيات 78 دول...</td>\n",
       "      <td>[{'text': 'بطولتين', 'answer_start': 394}]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arabic--3358420169913421088-0</td>\n",
       "      <td>من هو مكتشف المرو أو الكوارتز ؟</td>\n",
       "      <td>المرو أو الكوارتز  هو معدن يعود اكتشافه إلى ال...</td>\n",
       "      <td>[{'text': '(بيير كوري) وأخوه (جاك)', 'answer_s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arabic-6869798435672288559-0</td>\n",
       "      <td>كيف يتصل الجنين بالرحم ؟</td>\n",
       "      <td>المَشِيمَة أو الخَلاَص أو السُخْد (بالإنجليزي...</td>\n",
       "      <td>[{'text': 'عن طريق الحبل السري', 'answer_start...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arabic-1608369850259830544-0</td>\n",
       "      <td>ما هي المَشِيمَة أو الخَلاَص أو السُخْد؟</td>\n",
       "      <td>المَشِيمَة أو الخَلاَص أو السُخْد (بالإنجليزي...</td>\n",
       "      <td>[{'text': 'عضو دائري مسطح الشكل يتصل بالجنين ع...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arabic--7165672200361826550-0</td>\n",
       "      <td>أين يقع مسجد السلطان عبد المجيد؟</td>\n",
       "      <td>مسجد السلطان عبد المجيد، هو مسجد أثري تاريخي ف...</td>\n",
       "      <td>[{'text': 'مدينة جبيل اللبنانية', 'answer_star...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  \\\n",
       "0   arabic-2387335860751143628-1   \n",
       "1  arabic--3358420169913421088-0   \n",
       "2   arabic-6869798435672288559-0   \n",
       "3   arabic-1608369850259830544-0   \n",
       "4  arabic--7165672200361826550-0   \n",
       "\n",
       "                                            question  \\\n",
       "0  كم عدد مرات فوز الأوروغواي ببطولة كاس العالم ل...   \n",
       "1                    من هو مكتشف المرو أو الكوارتز ؟   \n",
       "2                           كيف يتصل الجنين بالرحم ؟   \n",
       "3           ما هي المَشِيمَة أو الخَلاَص أو السُخْد؟   \n",
       "4                   أين يقع مسجد السلطان عبد المجيد؟   \n",
       "\n",
       "                                             context  \\\n",
       "0  أقيمت البطولة 21 مرة، شارك في النهائيات 78 دول...   \n",
       "1  المرو أو الكوارتز  هو معدن يعود اكتشافه إلى ال...   \n",
       "2   المَشِيمَة أو الخَلاَص أو السُخْد (بالإنجليزي...   \n",
       "3   المَشِيمَة أو الخَلاَص أو السُخْد (بالإنجليزي...   \n",
       "4  مسجد السلطان عبد المجيد، هو مسجد أثري تاريخي ف...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0         [{'text': 'بطولتين', 'answer_start': 394}]     0  \n",
       "1  [{'text': '(بيير كوري) وأخوه (جاك)', 'answer_s...     1  \n",
       "2  [{'text': 'عن طريق الحبل السري', 'answer_start...     2  \n",
       "3  [{'text': 'عضو دائري مسطح الشكل يتصل بالجنين ع...     2  \n",
       "4  [{'text': 'مدينة جبيل اللبنانية', 'answer_star...     3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tydiqa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tydiqa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569666f4dc3983dab5624e989212c1d9d0cd1798</td>\n",
       "      <td>هل تزول الإصابة بمرض ذبابة الرمال عبر الوقت؟</td>\n",
       "      <td>تنتقل حمى الفواصد عن طريق لدغات حشرات من جنس ا...</td>\n",
       "      <td>[{'text': 'وتبقى مصابة بالفيروس مدى حياتها', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>561971f7978f678c3d1ba2a946036cdc131c4d49</td>\n",
       "      <td>ما الذي يتكون بالكامل تقريبا من خلايا نسيجية؟</td>\n",
       "      <td>تعد خلايا متن النبات خلايا حية لديها وظائف متن...</td>\n",
       "      <td>[{'text': 'الأوراق', 'answer_start': 194}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60ee75c50c8472be7cce1a24ee2cd7409ee4dd52</td>\n",
       "      <td>ما اسم نوع الخيوط الذي يحتوي على أجزاء من الكرفس؟</td>\n",
       "      <td>خلايا النسيج الغروي - تعيش خلايا النسيج الغروي...</td>\n",
       "      <td>[{'text': 'النسيج الغروي.', 'answer_start': 11...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6a636c91f7733165c92ed84864debf6bdbdf7d16</td>\n",
       "      <td>ما هو ناتج تميز الخلايا النباتية؟</td>\n",
       "      <td>تختلف الطبقات الرئيسية من الخلايا عن الخلايا ا...</td>\n",
       "      <td>[{'text': 'هياكل أنسجة الجذور والسيقان والأورا...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18a5c05701b7359fcd32a379b2ac4a9a5d7544de</td>\n",
       "      <td>أي نوع من النباتات يحتوي على النسيج الوعائي ال...</td>\n",
       "      <td>تكون خلايا النسيج الوعائي الخشبي  خلايا مطولةً...</td>\n",
       "      <td>[{'text': 'تراشيوفيت', 'answer_start': 320}]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  569666f4dc3983dab5624e989212c1d9d0cd1798   \n",
       "1  561971f7978f678c3d1ba2a946036cdc131c4d49   \n",
       "2  60ee75c50c8472be7cce1a24ee2cd7409ee4dd52   \n",
       "3  6a636c91f7733165c92ed84864debf6bdbdf7d16   \n",
       "4  18a5c05701b7359fcd32a379b2ac4a9a5d7544de   \n",
       "\n",
       "                                            question  \\\n",
       "0       هل تزول الإصابة بمرض ذبابة الرمال عبر الوقت؟   \n",
       "1      ما الذي يتكون بالكامل تقريبا من خلايا نسيجية؟   \n",
       "2  ما اسم نوع الخيوط الذي يحتوي على أجزاء من الكرفس؟   \n",
       "3                  ما هو ناتج تميز الخلايا النباتية؟   \n",
       "4  أي نوع من النباتات يحتوي على النسيج الوعائي ال...   \n",
       "\n",
       "                                             context  \\\n",
       "0  تنتقل حمى الفواصد عن طريق لدغات حشرات من جنس ا...   \n",
       "1  تعد خلايا متن النبات خلايا حية لديها وظائف متن...   \n",
       "2  خلايا النسيج الغروي - تعيش خلايا النسيج الغروي...   \n",
       "3  تختلف الطبقات الرئيسية من الخلايا عن الخلايا ا...   \n",
       "4  تكون خلايا النسيج الوعائي الخشبي  خلايا مطولةً...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0  [{'text': 'وتبقى مصابة بالفيروس مدى حياتها', '...     0  \n",
       "1         [{'text': 'الأوراق', 'answer_start': 194}]     1  \n",
       "2  [{'text': 'النسيج الغروي.', 'answer_start': 11...     2  \n",
       "3  [{'text': 'هياكل أنسجة الجذور والسيقان والأورا...     3  \n",
       "4       [{'text': 'تراشيوفيت', 'answer_start': 320}]     4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mlqa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mlqa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48344 entries, 0 to 48343\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        48344 non-null  object\n",
      " 1   question  48344 non-null  object\n",
      " 2   context   48344 non-null  object\n",
      " 3   answers   48344 non-null  object\n",
      " 4   c_id      48344 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_squad.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 693 entries, 0 to 692\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        693 non-null    int64 \n",
      " 1   question  693 non-null    object\n",
      " 2   context   693 non-null    object\n",
      " 3   answers   693 non-null    object\n",
      " 4   c_id      693 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 27.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_arcd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 921 entries, 0 to 920\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        921 non-null    object\n",
      " 1   question  921 non-null    object\n",
      " 2   context   921 non-null    object\n",
      " 3   answers   921 non-null    object\n",
      " 4   c_id      921 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 36.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_tydiqa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        517 non-null    object\n",
      " 1   question  517 non-null    object\n",
      " 2   context   517 non-null    object\n",
      " 3   answers   517 non-null    object\n",
      " 4   c_id      517 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 20.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_mlqa.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Removing ids column in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_squad.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48344 entries, 0 to 48343\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  48344 non-null  object\n",
      " 1   context   48344 non-null  object\n",
      " 2   answers   48344 non-null  object\n",
      " 3   c_id      48344 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_squad.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ما هو ASCII على أساس؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': 'الأبجدية الإنجليزية', 'answer_start...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>كم شخصيات محددة موجودة في كود ASCII؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '128 حرف ا محدد', 'answer_start': 58}]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كم عدد أحرف التحكم غير الطباعة؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '33 حرف ا تحكم ا غير الطباعة', 'answ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كم شخصيات قابلة للطباعة؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '95 حرف ا قابلا للطباعة', 'answer_st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ما هو الفضاء المعروف أيضا باسم ماذا؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': 'رسم ا غير مرئي 223', 'answer_start'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>لماذا تم ترميز الرمز بحيث تكون معظم الرموز معًا؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': 'لسهولة تحديد الهوية', 'answer_start...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>كم عدد المراكز في العمودين الأولين؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': '32 موضع', 'answer_start': 128}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ما الذي قررت اللجنة أنه مهم؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': 'دعم الأحرف الهجائية الكبيرة المكونة...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>أين كان الحرف ألف يضع في الموقف؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': '41hex', 'answer_start': 757}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>تم دمج ASCII في ما مجموعة الأحرف الأخرى؟</td>\n",
       "      <td>تم دمج ASCII في مجموعة أحرف ونيكودي كأول 128 ر...</td>\n",
       "      <td>[{'text': 'ونيكودي', 'answer_start': 28}]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question  \\\n",
       "0                             ما هو ASCII على أساس؟   \n",
       "1              كم شخصيات محددة موجودة في كود ASCII؟   \n",
       "2                   كم عدد أحرف التحكم غير الطباعة؟   \n",
       "3                          كم شخصيات قابلة للطباعة؟   \n",
       "4              ما هو الفضاء المعروف أيضا باسم ماذا؟   \n",
       "5  لماذا تم ترميز الرمز بحيث تكون معظم الرموز معًا؟   \n",
       "6               كم عدد المراكز في العمودين الأولين؟   \n",
       "7                      ما الذي قررت اللجنة أنه مهم؟   \n",
       "8                  أين كان الحرف ألف يضع في الموقف؟   \n",
       "9          تم دمج ASCII في ما مجموعة الأحرف الأخرى؟   \n",
       "\n",
       "                                             context  \\\n",
       "0  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "1  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "2  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "3  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "4  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "5  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "6  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "7  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "8  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "9  تم دمج ASCII في مجموعة أحرف ونيكودي كأول 128 ر...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0  [{'text': 'الأبجدية الإنجليزية', 'answer_start...     0  \n",
       "1   [{'text': '128 حرف ا محدد', 'answer_start': 58}]     0  \n",
       "2  [{'text': '33 حرف ا تحكم ا غير الطباعة', 'answ...     0  \n",
       "3  [{'text': '95 حرف ا قابلا للطباعة', 'answer_st...     0  \n",
       "4  [{'text': 'رسم ا غير مرئي 223', 'answer_start'...     0  \n",
       "5  [{'text': 'لسهولة تحديد الهوية', 'answer_start...     1  \n",
       "6         [{'text': '32 موضع', 'answer_start': 128}]     1  \n",
       "7  [{'text': 'دعم الأحرف الهجائية الكبيرة المكونة...     1  \n",
       "8           [{'text': '41hex', 'answer_start': 757}]     1  \n",
       "9          [{'text': 'ونيكودي', 'answer_start': 28}]     2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tried to make something but found out it is the same as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assign a unique question_id to each question\n",
    "# df_squad['question_id'] = df_squad.index\n",
    "\n",
    "# # Function to get the context and its associated question_ids\n",
    "# def get_context_question_ids(df):\n",
    "#     grouped = df.groupby('c_id').agg(\n",
    "#         context=('context', 'first'),\n",
    "#         question_ids=('question_id', list)\n",
    "#     ).reset_index()\n",
    "#     return grouped\n",
    "\n",
    "# # Generate the new DataFrame\n",
    "# C_Q_df = get_context_question_ids(df_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_Q_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function to get the assigned questions\n",
    "# def get_assigned_questions(df, c_id):\n",
    "#     filtered = df[df['c_id'] == c_id]\n",
    "#     Q = filtered['question'].tolist()\n",
    "#     A = filtered['answers'].tolist()\n",
    "#     return Q, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = []\n",
    "\n",
    "# for _, row in C_Q_df.iterrows():\n",
    "#     context = row['context']\n",
    "#     question_ids = row['question_ids']\n",
    "    \n",
    "#     # For each question_id in the context, fetch the corresponding question\n",
    "#     for question_id in question_ids:\n",
    "#         question = df_squad.loc[df_squad['question_id'] == question_id, 'question'].values[0]\n",
    "#         training_data.append({\n",
    "#             'context': context,\n",
    "#             'question': question,\n",
    "#             'answers': df_squad.loc[df_squad['question_id'] == question_id, 'answers'].values[0]\n",
    "#         })\n",
    "        \n",
    "# training_df = pd.DataFrame(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Arrange all data frames by their c_id column, to make same context after each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Combine all datasets as they already contain the same columns (pd.concat([df1, df2], ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text cleaning functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    # remove diacritics from the text as it may confuse the model\n",
    "    return re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_arabic(text):\n",
    "    # Remove non-Arabic characters, keeping essential punctuation\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s؟]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punk(text):\n",
    "    # Remove punctuation\n",
    "    arabic_punctuation = r'[،؛؟…!\"#$%&\\'()*+,-./:;<=>@^_`{|}~]'\n",
    "    text = re.sub(arabic_punctuation, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_alf(text):\n",
    "#     alf = 'ا'\n",
    "#     text = re.sub(alf, '', text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'خاء', 'هَيْهات', 'كيفما', 'كأي', 'بَسْ', 'ثمَّ', 'أربعمائة', 'هيهات', 'ستمائة', 'أنشأ', 'حرى', 'حبيب', 'ي', 'علًّ', 'جمعة', 'ذين', 'لا سيما', 'حزيران', 'نبَّا', 'إذا', 'فإذا', 'إلَيْكَ', 'لكما', 'تسع', 'ريال', 'حيثما', 'ثمان', 'أوت', 'وَيْ', 'ثلاث', 'بكن', 'تخذ', 'مرّة', 'من', 'ر', 'بك', 'تلكم', 'ألف', 'اللاتي', 'أيّان', 'بلى', 'بكم', 'ثمانمئة', 'تبدّل', 'به', 'أولالك', 'لدن', 'هو', 'هاء', 'قام', 'وهب', 'أغسطس', 'آنفا', 'هاتان', 'هلا', 'أقل', 'إذ', 'سابع', 'ث', 'هَذَيْنِ', 'وإذ', 'صبرا', 'لبيك', 'اللتين', 'آناء', 'فإن', 'ذواتي', 'تعسا', 'ظلّ', 'مارس', 'فمن', 'مافتئ', 'شباط', 'فرادى', 'خلف', 'عشر', 'وراءَك', 'خال', 'ذلكما', 'آمينَ', 'تسعمائة', 'أربعمئة', 'قاف', 'ته', 'ذواتا', 'ولا', 'ع', 'أنا', 'أى', 'دينار', 'يوليو', 'قطّ', 'في', 'ضحوة', 'إي', 'هللة', 'مساء', 'خبَّر', 'قبل', 'لما', 'ليرة', 'أقبل', 'ذوا', 'أنتم', 'عين', 'كأن', 'بيد', 'تموز', 'ذَيْنِ', 'ثلاثمئة', 'حاشا', 'ّأيّان', 'أجمع', 'ماذا', 'أل', 'الذي', 'رأى', 'أنّى', 'أعلم', 'جميع', 'نعم', 'غالبا', 'ذه', 'والذين', 'جعل', 'إيهٍ', 'لعلَّ', 'تِه', 'ف', 'لو', 'هَؤلاء', 'كذا', 'مهما', 'ثمّة', 'ذِه', 'نفس', 'نيسان', 'حيث', 'دواليك', 'صاد', 'ممن', 'كلما', 'بي', 'د', 'أهلا', 'أولاء', 'عليك', 'هذه', 'أوّهْ', 'إى', 'اربعين', 'أوشك', 'م', 'انقلب', 'تينك', 'متى', 'تلقاء', 'نا', 'طرا', 'فضلا', 'شتان', 'فاء', 'والذي', 'على', 'بطآن', 'ثالث', 'تِي', 'أولئك', 'مائة', 'ليستا', 'إياهن', 'منه', 'ذلكم', 'عشرة', 'ت', 'عيانا', 'سرا', 'باء', 'آض', 'إزاء', 'لولا', 'ما انفك', 'ليس', 'خلا', 'ظاء', 'عاد', 'بَلْهَ', 'دون', 'سبت', 'هلّا', 'ترك', 'سبعمئة', 'سبعمائة', 'أيضا', 'غين', 'كاف', 'كي', 'إيه', 'اتخذ', 'حيَّ', 'أخو', 'هَاتَيْنِ', 'خاصة', 'إياهما', 'حسب', 'أحد', 'ولو', 'يناير', 'مادام', 'عن', 'دال', 'إياكم', 'هاك', 'ارتدّ', 'إحدى', 'ذ', 'شمال', 'بخ', 'اخلولق', 'أم', 'اللائي', 'كلا', 'صراحة', 'اللتان', 'هم', 'إليكَ', 'بعض', 'عما', 'سادس', 'فو', 'حاء', 'أرى', 'حار', 'لكن', 'تسعين', 'بما', 'ضاد', 'أمامك', 'فيم', 'طفق', 'بكما', 'مكانكم', 'ذِي', 'دونك', 'خميس', 'إليكنّ', 'عليه', 'وهو', 'عدا', 'كما', 'عسى', 'اللذان', 'التي', 'آها', 'ذا', 'أربع', 'سبعين', 'لئن', 'ثلاثاء', 'ورد', 'هيت', 'ثلاثة', 'آي', 'ذلك', 'اللتيا', 'شَتَّانَ', 'أيلول', 'كثيرا', 'إنا', 'ين', 'همزة', 'ومن', 'وإن', 'أنتِ', 'ذو', 'تعلَّم', 'كان', 'فيه', 'كلاهما', 'بغتة', 'أوه', 'هنالك', 'تشرين', 'لها', 'عوض', 'زاي', 'كليكما', 'فبراير', 'لهما', 'صبر', 'بمن', 'رابع', 'عاشر', 'بها', 'جانفي', 'لسنا', 'هاهنا', 'ما أفعله', 'واهاً', 'إليكن', 'هل', 'تارة', 'لهم', 'سوى', 'هنا', 'لكم', 'عَدَسْ', 'منها', 'بين', 'الذين', 'يمين', 'ذينك', 'بؤسا', 'غادر', 'هَذِه', 'بنا', 'ثلاثمائة', 'يا', 'سرعان', 'صهْ', 'صهٍ', 'نحو', 'و', 'حدَث', 'مليم', 'أربعة', 'كأيّن', 'نَخْ', 'علق', 'تين', 'ثامن', 'خمسون', 'هؤلاء', 'نيف', 'لستم', 'آب', 'ميم', 'خلافا', 'لكنَّ', 'لاسيما', 'حين', 'ة', 'أفٍّ', 'ذهب', 'أمس', 'اللواتي', 'فلا', 'طَق', 'إليكما', 'تسعمئة', 'لكي', 'تفعلون', 'لعمر', 'شرع', 'أما', 'طاء', 'تانِك', 'أبدا', 'ابتدأ', 'علم', 'قد', 'كليهما', 'أكثر', 'كأنما', 'إما', 'لعل', 'ذال', 'أينما', 'ما', 'فوق', 'شبه', 'جير', 'عدَّ', 'عل', 'ولكن', 'آذار', 'هَذا', 'هيا', 'لستن', 'لنا', 'لم', 'فيفري', 'ياء', 'سبع', 'إذما', 'أكتوبر', 'لام', 'كلَّا', 'ستمئة', 'خمسمائة', 'إنما', 'إذاً', 'إياي', 'بهما', 'ثماني', 'لدى', 'بسّ', 'ريث', 'عجبا', 'ثان', 'ثمنمئة', 'كم', 'حمو', 'ئ', 'هيّا', 'ستون', 'تجاه', 'ض', 'تفعلين', 'لستما', 'إن', 'بعدا', 'ها', 'نون', 'أين', 'هَذانِ', 'تاسع', 'إلا', 'هن', 'هناك', 'لكيلا', 'أمامكَ', 'دولار', 'كسا', 'ص', 'كن', 'هاتين', 'ألا', 'إلّا', 'هذان', 'ثمانية', 'خ', 'وإذا', 'تحت', 'له', 'كيت', 'أنتن', 'ك', 'أضحى', 'الألاء', 'أطعم', 'أمد', 'واحد', 'أبريل', 'ماي', 'كرب', 'حجا', 'رزق', 'إياهم', 'هذي', 'هَاتِه', 'ل', 'درى', 'حقا', 'تاء', 'وجد', 'خامس', 'تَيْنِ', 'نَّ', 'حمدا', 'فيما', 'حبذا', 'لن', 'كأنّ', 'أمسى', 'إيانا', 'ذلكن', 'أمّا', 'مايو', 'تفعلان', 'إمّا', 'مئة', 'شيكل', 'صار', 'ذيت', 'غدا', 'كذلك', 'خمس', 'يفعلون', 'رُبَّ', 'أمام', 'اثني', 'مكانكما', 'صباح', 'فلس', 'ثلاثين', 'رويدك', 'ثمانون', 'ست', 'سحقا', 'يورو', 'أُفٍّ', 'بهن', 'سنتيم', 'أنتما', 'ثاني', 'بعد', 'قلما', 'ءَ', 'ثمة', 'هبّ', 'معاذ', 'سقى', 'بل', 'لوما', 'بضع', 'إياكن', 'واو', 'كلتا', 'فيها', 'عشرين', 'أنًّ', 'ى', 'ثم', 'ذي', 'هما', 'غير', 'أو', 'إياها', 'جوان', 'زود', 'لست', 'كأيّ', 'عشرون', 'هذين', 'ء', 'ليست', 'هي', 'سبعة', 'صدقا', 'أنبأ', 'يفعلان', 'جلل', 'ش', 'هَذِي', 'ديسمبر', 'أفعل به', 'منذ', 'حَذارِ', 'أ', 'ما برح', 'نحن', 'ق', 'أف', 'هذا', 'وما', 'تسعة', 'ز', 'إليكم', 'اللذين', 'ثمّ', 'لمّا', 'سوف', 'ألفى', 'اربعون', 'أفريل', 'غ', 'إنه', 'بخٍ', 'تي', 'ظ', 'كِخ', 'ليسا', 'كل', 'جويلية', 'درهم', 'كى', 'رجع', 'أخذ', 'سبحان', 'حادي', 'حمٌ', 'أعطى', 'ج', 'ؤ', 'لات', 'طالما', 'ثمانين', 'استحال', 'ذان', 'خمسين', 'لسن', 'بئس', 'يوان', 'تلك', 'خمسمئة', 'مازال', 'سمعا', 'مئتان', 'إياكما', 'ذاك', 'آهِ', 'ح', 'آهاً', 'وا', 'أيا', 'أصلا', 'كانون', 'آ', 'سبعون', 'ذانِ', 'بهم', 'أبٌ', 'لا', 'ثلاثون', 'آه', 'لهن', 'طاق', 'سبتمبر', 'هَاتانِ', 'آهٍ', 'ليت', 'حاي', 'ثاء', 'ظنَّ', 'ه', 'مثل', 'ب', 'ساء', 'بس', 'مكانكنّ', 'اثنا', 'أسكن', 'ستة', 'إذن', 'أول', 'ط', 'هلم', 'غداة', 'لك', 'جيم', 'زعم', 'أصبح', 'سين', 'عند', 'مذ', 'اثنين', 'راح', 'كأين', 'حتى', 'أربعاء', 'يونيو', 'إياه', 'تسعون', 'أيّ', 'هَجْ', 'مما', 'فلان', 'تحوّل', 'كيف', 'ا', 'أجل', 'ن', 'عامة', 'هاتي', 'أخٌ', 'هكذا', 'راء', 'تانِ', 'نوفمبر', 'أخبر', 'مكانَك', 'إلى', 'مع', 'خمسة', 'لكنما', 'أي', 'س', 'قاطبة', 'إليك', 'ستين', 'أيار', 'أنى', 'اثنان', 'ليسوا', 'أبو', 'هاته', 'أن', 'إنَّ', 'تلكما', 'بات', 'هَاتِي', 'مه', 'كلّما', 'إياك', 'ذانك', 'أيها', 'هاكَ', 'الآن', 'قرش', 'لي', 'وُشْكَانَ', 'شين', 'أنت', 'الألى', 'شتانَ', 'كاد', 'ذات', 'جنيه', 'بماذا', 'انبرى'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('arabic'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_numbers(text, replace_with=None):\n",
    "    if replace_with:\n",
    "        text = re.sub(r'\\d+', replace_with, text)\n",
    "    else:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    sentences = re.split(r'[؟.!؟]', text)\n",
    "    # Remove empty sentences and strip spaces\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"aubmindlab/bert-base-arabertv2\"   # replace the model with another when trying\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example Arabic text\n",
    "# text = \"مرحبا بك في عالم معالجة اللغة الطبيعية\"\n",
    "\n",
    "# # Tokenize the text\n",
    "# encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "# print(\"text after tokenize\", encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_tokenize(text):\n",
    "    return tokenizer(text, return_tensors=\"pt\") # pt -> return tensors for pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ما هو ASCII على أساس؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': 'الأبجدية الإنجليزية', 'answer_start...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>كم شخصيات محددة موجودة في كود ASCII؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '128 حرف ا محدد', 'answer_start': 58}]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كم عدد أحرف التحكم غير الطباعة؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '33 حرف ا تحكم ا غير الطباعة', 'answ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كم شخصيات قابلة للطباعة؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': '95 حرف ا قابلا للطباعة', 'answer_st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ما هو الفضاء المعروف أيضا باسم ماذا؟</td>\n",
       "      <td>يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...</td>\n",
       "      <td>[{'text': 'رسم ا غير مرئي 223', 'answer_start'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>لماذا تم ترميز الرمز بحيث تكون معظم الرموز معًا؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': 'لسهولة تحديد الهوية', 'answer_start...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>كم عدد المراكز في العمودين الأولين؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': '32 موضع', 'answer_start': 128}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ما الذي قررت اللجنة أنه مهم؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': 'دعم الأحرف الهجائية الكبيرة المكونة...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>أين كان الحرف ألف يضع في الموقف؟</td>\n",
       "      <td>تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...</td>\n",
       "      <td>[{'text': '41hex', 'answer_start': 757}]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>تم دمج ASCII في ما مجموعة الأحرف الأخرى؟</td>\n",
       "      <td>تم دمج ASCII في مجموعة أحرف ونيكودي كأول 128 ر...</td>\n",
       "      <td>[{'text': 'ونيكودي', 'answer_start': 28}]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question  \\\n",
       "0                             ما هو ASCII على أساس؟   \n",
       "1              كم شخصيات محددة موجودة في كود ASCII؟   \n",
       "2                   كم عدد أحرف التحكم غير الطباعة؟   \n",
       "3                          كم شخصيات قابلة للطباعة؟   \n",
       "4              ما هو الفضاء المعروف أيضا باسم ماذا؟   \n",
       "5  لماذا تم ترميز الرمز بحيث تكون معظم الرموز معًا؟   \n",
       "6               كم عدد المراكز في العمودين الأولين؟   \n",
       "7                      ما الذي قررت اللجنة أنه مهم؟   \n",
       "8                  أين كان الحرف ألف يضع في الموقف؟   \n",
       "9          تم دمج ASCII في ما مجموعة الأحرف الأخرى؟   \n",
       "\n",
       "                                             context  \\\n",
       "0  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "1  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "2  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "3  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "4  يعتمد ASCII أساس ا على الأبجدية الإنجليزية ، و...   \n",
       "5  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "6  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "7  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "8  تم تزيين الرمز نفسه بحيث تكون معظم رموز التحكم...   \n",
       "9  تم دمج ASCII في مجموعة أحرف ونيكودي كأول 128 ر...   \n",
       "\n",
       "                                             answers  c_id  \n",
       "0  [{'text': 'الأبجدية الإنجليزية', 'answer_start...     0  \n",
       "1   [{'text': '128 حرف ا محدد', 'answer_start': 58}]     0  \n",
       "2  [{'text': '33 حرف ا تحكم ا غير الطباعة', 'answ...     0  \n",
       "3  [{'text': '95 حرف ا قابلا للطباعة', 'answer_st...     0  \n",
       "4  [{'text': 'رسم ا غير مرئي 223', 'answer_start'...     0  \n",
       "5  [{'text': 'لسهولة تحديد الهوية', 'answer_start...     1  \n",
       "6         [{'text': '32 موضع', 'answer_start': 128}]     1  \n",
       "7  [{'text': 'دعم الأحرف الهجائية الكبيرة المكونة...     1  \n",
       "8           [{'text': '41hex', 'answer_start': 757}]     1  \n",
       "9          [{'text': 'ونيكودي', 'answer_start': 28}]     2  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: First see what preprocessing techniques we will use for the text for questions, contexts, answers(only text cleaning(i think))\n",
    "\n",
    "# FIXME: Squad data is cleaned from the diacritics and been replaced with space, how to fix:\n",
    "#           1- combine every ا to its previous word as the original word but some words may corrupt\n",
    "#           2- combine every ى and أ with next word when coming in first\n",
    "#           3- remove every alone character but this also maya corrupt some words as some words splitted from the diacritics (except for ب, ك and و)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2 approaches to be taken on the datasets:\n",
    "#              1- apply the fixme changes to squad data\n",
    "#              2- try the nlp preprocessing techniques to the other datasets (arcd, tydiqa, mlqa) and see if there any problems in them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_squad['processed_question'] = df_squad['question'].apply(remove_diacritics).apply(remove_extra_spaces).apply(remove_punk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* i tried to apply text clean function but i think it made some question not understandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              ما هو ASCII على أساس\n",
       "1               كم شخصيات محددة موجودة في كود ASCII\n",
       "2                    كم عدد أحرف التحكم غير الطباعة\n",
       "3                           كم شخصيات قابلة للطباعة\n",
       "4               ما هو الفضاء المعروف أيضا باسم ماذا\n",
       "5    لماذا تم ترميز الرمز بحيث تكون معظم الرموز معا\n",
       "6                كم عدد المراكز في العمودين الأولين\n",
       "7                       ما الذي قررت اللجنة أنه مهم\n",
       "8                   أين كان الحرف ألف يضع في الموقف\n",
       "9           تم دمج ASCII في ما مجموعة الأحرف الأخرى\n",
       "Name: processed_question, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad['processed_question'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = []\n",
    "for i in range(len(df_squad['question'])):\n",
    "    c = df_squad['question'][i].count('؟')\n",
    "    list.append(c)\n",
    "    # print(df_squad['question'][0])\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['خلق الغموض المتأصل للعديد من أحرف التحكم ، جنبا إلى جنب مع استخدامها التاريخي ، مشاكل عند نقل الملفات النص العادي بين الأنظمة . وأفضل مثال على ذلك هو مشكلة الخط الجديد في أنظمة التشغيل المختلفة . تطلبت أجهزة تيليتيبي إنهاء سطر النص بكل من كاررياغي ريتورن الذي يحرك رأس الطباعة إلى بداية السطر و ليني فيد الذي يعمل على دفع خط الورق بدون تحريك رأس الطباعة . يأتي اسم كاررياغي ريتورن من حقيقة أن الآلة الكاتبة التي تحمل الورقة قد تحركت على آلة كاتبة يدوية ، في حين بقي الوضع الذي ضربت فيه الشظايا الشريطية ثابت ا . اضطرت إلى دفع كامل العربة أرجعت إلى اليمين لوضع الهامش الأيسر للورقة للسطر التالي .',\n",
       " 'خلق الغموض المتأصل للعديد من أحرف التحكم ، جنبا إلى جنب مع استخدامها التاريخي ، مشاكل عند نقل الملفات النص العادي بين الأنظمة . وأفضل مثال على ذلك هو مشكلة الخط الجديد في أنظمة التشغيل المختلفة . تطلبت أجهزة تيليتيبي إنهاء سطر النص بكل من كاررياغي ريتورن الذي يحرك رأس الطباعة إلى بداية السطر و ليني فيد الذي يعمل على دفع خط الورق بدون تحريك رأس الطباعة . يأتي اسم كاررياغي ريتورن من حقيقة أن الآلة الكاتبة التي تحمل الورقة قد تحركت على آلة كاتبة يدوية ، في حين بقي الوضع الذي ضربت فيه الشظايا الشريطية ثابت ا . اضطرت إلى دفع كامل العربة أرجعت إلى اليمين لوضع الهامش الأيسر للورقة للسطر التالي .',\n",
       " 'خلق الغموض المتأصل للعديد من أحرف التحكم ، جنبا إلى جنب مع استخدامها التاريخي ، مشاكل عند نقل الملفات النص العادي بين الأنظمة . وأفضل مثال على ذلك هو مشكلة الخط الجديد في أنظمة التشغيل المختلفة . تطلبت أجهزة تيليتيبي إنهاء سطر النص بكل من كاررياغي ريتورن الذي يحرك رأس الطباعة إلى بداية السطر و ليني فيد الذي يعمل على دفع خط الورق بدون تحريك رأس الطباعة . يأتي اسم كاررياغي ريتورن من حقيقة أن الآلة الكاتبة التي تحمل الورقة قد تحركت على آلة كاتبة يدوية ، في حين بقي الوضع الذي ضربت فيه الشظايا الشريطية ثابت ا . اضطرت إلى دفع كامل العربة أرجعت إلى اليمين لوضع الهامش الأيسر للورقة للسطر التالي .',\n",
       " 'خلق الغموض المتأصل للعديد من أحرف التحكم ، جنبا إلى جنب مع استخدامها التاريخي ، مشاكل عند نقل الملفات النص العادي بين الأنظمة . وأفضل مثال على ذلك هو مشكلة الخط الجديد في أنظمة التشغيل المختلفة . تطلبت أجهزة تيليتيبي إنهاء سطر النص بكل من كاررياغي ريتورن الذي يحرك رأس الطباعة إلى بداية السطر و ليني فيد الذي يعمل على دفع خط الورق بدون تحريك رأس الطباعة . يأتي اسم كاررياغي ريتورن من حقيقة أن الآلة الكاتبة التي تحمل الورقة قد تحركت على آلة كاتبة يدوية ، في حين بقي الوضع الذي ضربت فيه الشظايا الشريطية ثابت ا . اضطرت إلى دفع كامل العربة أرجعت إلى اليمين لوضع الهامش الأيسر للورقة للسطر التالي .']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad['processed_context'] = df_squad['context'].apply(remove_punk)\n",
    "df_squad['context'][df_squad['c_id'] == 22].tolist()\n",
    "# df_squad['context'].head(1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_contexts_df = pd.DataFrame(df_squad['context'].unique().tolist(), columns=[\"unique context\"])\n",
    "unique_contexts_df.to_csv(\"unique_context.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ما هو ASCII على أساس'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad.loc[0, 'question'] = df_squad['question'][0].replace('؟', '')\n",
    "df_squad['question'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pre-trained Arabic T5 model and tokenizer\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1782\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1784\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1781\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1779\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1793\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1793\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1797\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\t5\\modeling_t5.py:40\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     BaseModelOutput,\n\u001b[0;32m     33\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     DUMMY_INPUTS,\n\u001b[0;32m     44\u001b[0m     DUMMY_MASK,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileConfig, GenerationConfig, GenerationMixin\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     translate_to_torch_parallel_style,\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, HfQuantizer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_rt_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTDetrForObjectDetectionLoss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     HungarianMatcher,\n\u001b[0;32m      8\u001b[0m     ImageLoss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     sigmoid_focal_loss,\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\image_transforms.py:50\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py:47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lookup\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\lite\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpsSet \u001b[38;5;66;03m# line: 170\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m toco_convert \u001b[38;5;66;03m# line: 1083\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\lite\\experimental\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.lite.experimental namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m authoring\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelAnalyzer \u001b[38;5;28;01mas\u001b[39;00m Analyzer \u001b[38;5;66;03m# line: 35\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpResolverType \u001b[38;5;66;03m# line: 303\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\lite\\experimental\\authoring\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.lite.experimental.authoring namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauthoring\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauthoring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compatible \u001b[38;5;66;03m# line: 263\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_wrapper \u001b[38;5;28;01mas\u001b[39;00m _module_wrapper\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m], _module_wrapper\u001b[38;5;241m.\u001b[39mTFModuleWrapper):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\lite\\python\\authoring\\authoring.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export \u001b[38;5;28;01mas\u001b[39;00m _tf_export\n\u001b[0;32m     47\u001b[0m _CUSTOM_OPS_HDR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom ops: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\lite\\python\\lite.py:53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_phase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SubComponent\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert_saved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m freeze_saved_model \u001b[38;5;28;01mas\u001b[39;00m _freeze_saved_model\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interpreter  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_delegate  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpResolverType  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained Arabic T5 model and tokenizer\n",
    "model_name = \"aubmindlab/aragpt2-large\"  # Arabic T5 or a suitable Arabic model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs = tokenizer(examples['output'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "train_dataset = training_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define the Trainer arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maubmindlab/aragpt2-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Function to clean the text\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Remove unnecessary characters, HTML tags, or special symbols\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1639\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1637\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"aubmindlab/aragpt2-large\")\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary characters, HTML tags, or special symbols\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the context and questions\n",
    "df['context'] = df['context'].apply(clean_text)\n",
    "df['question'] = df['question'].apply(clean_text)\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['context'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs = tokenizer(examples['question'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_data = df.apply(tokenize_function, axis=1)\n",
    "\n",
    "# Convert to a suitable format for the Trainer\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_data.items()}\n",
    "        return item\n",
    "\n",
    "train_dataset = QADataset(tokenized_data)\n",
    "\n",
    "# Now you can use the train_dataset with the Trainer for training\n",
    "# Define the Trainer arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(context, question):\n",
    "    input_text = f\"Context: {context} Question: {question}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(inputs['input_ids'])\n",
    "        \n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "context = \"This is context 1\"\n",
    "question = \"What is the answer to Q1?\"\n",
    "answer = generate_answer(context, question)\n",
    "print(\"Generated Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
